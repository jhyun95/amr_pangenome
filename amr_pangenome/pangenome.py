#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Created on Thu Mar 12 10:18:29 2020

@author: jhyun95

Pan-genome construction tools including consolidating redundant sequences,
gene sequence cluster identification by CD-Hit, and constructing gene/allele tables.
Refer to build_cds_pangenome() and build_upstream_pangenome().

Pan-genome feature nomenclature:
<name>_C# = Gene/CDS cluster
<name>_C#A# = Gene/CDS sequence variant "allele"
<name>_C#U# = Gene/CDS 5'UTR variant "upstream"
<name>_C#D# = Gene/CDS 3'UTR variant "downstream"
<name>_T# = Non-coding transcript/RNA cluster
<name>_T#A# = Noncoding transcript/RNA sequence variant
"""

import os, shutil, urllib
import subprocess as sp
import hashlib 
import collections

import pandas as pd
import numpy as np
import scipy.sparse

CLUSTER_TYPES = {'cds':'C', 'noncoding':'T'}
VARIANT_TYPES = {'allele':'A', 'upstream':'U', 'downstream':'D'}
CLUSTER_TYPES_REV = {v:k for k,v in CLUSTER_TYPES.items()}
VARIANT_TYPES_REV = {v:k for k,v in VARIANT_TYPES.items()}
DNA_COMPLEMENT = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 
              'W': 'W', 'S': 'S', 'R': 'Y', 'Y': 'R', 
              'M': 'K', 'K': 'M', 'N': 'N'}
for bp in DNA_COMPLEMENT.keys():
    DNA_COMPLEMENT[bp.lower()] = DNA_COMPLEMENT[bp].lower()

    
def build_cds_pangenome(genome_faa_paths, output_dir, name='Test', 
                        cdhit_args={'-n':5, '-c':0.8}, fastasort_path=None):
    ''' 
    Constructs a pan-genome based on protein sequences with the following steps:
    1) Merge FAA files for genomes of interest into a non-redundant list
    2) Cluster CDS by sequence into putative genes using CD-Hit
    3) Rename non-redundant CDS as <name>_C#A#, referring to cluster and allele number
    4) Compile allele/gene membership into binary allele x genome and gene x genome tables
    
    Generates eight files within output_dir:
    1) <name>_strain_by_allele.pickle.gz, binary allele x genome table with SparseArray structure
    2) <name>_strain_by_gene.pickle.gz, binary gene x genome table with SparseArray structure
    1) <name>_strain_by_allele.csv.gz, binary allele x genome table as flat file
    2) <name>_strain_by_gene.csv.gz, binary gene x genome table as flat file
    3) <name>_nr.faa, all non-redundant CDSs observed, with headers <name>_C#A#
    4) <name>_nr.faa.cdhit.clstr, CD-Hit output file from clustering
    5) <name>_allele_names.tsv, mapping between <name>_C#A# to original CDS headers
    6) <name>_redundant_headers.tsv, lists of headers sharing the same CDS, with the
        representative header relevant to #5 listed first for each group.
    7) <name>_missing_headers.txt, lists headers for original entries missing sequences
    
    Parameters
    ----------
    genome_faa_paths : list 
        FAA files containing CDSs for genomes of interest. Genome 
        names are inferred from these FAA file paths.
    output_dir : str
        Path to directory to generate outputs and intermediates.
    name : str
        Header to prepend to all output files and allele names (default 'Test')
    cdhit_args : dict
        Alignment arguments to pass CD-Hit, other than -i, -o, and -d
        (default {'-n':5, '-c':0.8})
    fastasort_path : str
        Path to Exonerate's fastasort binary, optionally for sorting
        final FAA files (default None)
        
    Returns 
    -------
    df_alleles : pd.DataFrame
        Binary allele x genome table
    df_genes : pd.DataFrame
        Binary gene x genome table
    '''
    
    ''' Merge FAAs into one file with non-redundant sequences '''
    print 'Identifying non-redundant CDS sequences...'
    output_nr_faa = output_dir + '/' + name + '_nr.faa' # final non-redundant FAA files
    output_shared_headers = output_dir + '/' + name + '_redundant_headers.tsv' # records headers that have the same sequence
    output_missing_headers = output_dir + '/' + name + '_missing_headers.txt' # records headers without any seqeunce
    output_nr_faa = output_nr_faa.replace('//','/')
    output_shared_headers = output_shared_headers.replace('//','/')
    output_missing_headers = output_missing_headers.replace('//','/')
    non_redundant_seq_hashes, missing_headers = consolidate_seqs(
        genome_faa_paths, output_nr_faa, output_shared_headers, output_missing_headers)
    # maps sequence hash to headers of that sequence, in order observed
    
    ''' Apply CD-Hit to non-redundant CDS sequences '''
    output_nr_faa_copy = output_nr_faa + '.cdhit' # temporary FAA copy generated by CD-Hit
    output_nr_clstr = output_nr_faa + '.cdhit.clstr' # cluster file generated by CD-Hit
    cluster_with_cdhit(output_nr_faa, output_nr_faa_copy, cdhit_args)
    os.remove(output_nr_faa_copy) # delete CD-hit copied sequences
    
    ''' Extract genes and alleles, rename unique sequences as <name>_C#A# '''
    output_allele_names = output_dir + '/' + name + '_allele_names.tsv' # allele names vs non-redundant headers
    output_allele_names = output_allele_names.replace('//','/')
    header_to_allele = rename_genes_and_alleles(
        output_nr_clstr, output_nr_faa, output_nr_faa, 
        output_allele_names, name=name, cluster_type='cds',
        shared_headers_file=output_shared_headers,
        fastasort_path=fastasort_path)
    # maps original headers to short names <name>_C#A#
    
    ''' Process gene/allele membership into binary tables '''    
    df_alleles, df_genes = build_genetic_feature_tables(
        output_nr_clstr, genome_faa_paths, name,
        cluster_type='cds', header_to_allele=header_to_allele)
    
    ''' Save tables as PICKLE.GZ (preserve SparseArrays) and CSV.GZ (backup flat file) '''
    output_allele_table = output_dir + '/' + name + '_strain_by_allele'
    output_gene_table = output_dir + '/' + name + '_strain_by_gene'
    output_allele_table = output_allele_table.replace('//','/')
    output_gene_table = output_gene_table.replace('//','/')
    output_allele_csv = output_allele_table + '.csv.gz'
    output_gene_csv = output_gene_table + '.csv.gz'
    output_allele_pickle = output_allele_table + '.pickle.gz'
    output_gene_pickle = output_gene_table + '.pickle.gz'
    print 'Saving', output_allele_pickle, '...'
    df_alleles.to_pickle(output_allele_pickle)
    print 'Saving', output_allele_csv, '...'
    df_alleles.to_csv(output_allele_csv)
    print 'Saving', output_gene_pickle, '...'
    df_genes.to_pickle(output_gene_pickle)
    print 'Saving', output_gene_csv, '...'
    df_genes.to_csv(output_gene_csv)
    
    return df_alleles, df_genes


def build_noncoding_pangenome(genome_data, output_dir, name='Test', flanking=(0,0),
                              allowed_features=['transcript', 'tRNA', 'rRNA', 'misc_binding'],
                              cdhit_args={'-n':5, '-c':0.8}, fastasort_path=None):
    ''' 
    Constructs a pan-genome based on noncoding sequences with the following steps:
    1) Extract non-coding transcripts (optionally with flanking NTs) based on FNA/GFF pairs
    2) Cluster CDS by sequence into putative transcripts using CD-HIT-EST
    3) Rename non-redundant transcript as <name>_T#A#, referring to transcript cluster and allele number
    4) Compile allele/transcript membership into binary transcript allele x genome and transcript x genome tables
    
    Generates eight files within output_dir:
    1) <name>_strain_by_noncoding_allele.pickle.gz, binary allele x genome table with SparseArray structure
    2) <name>_strain_by_noncoding.pickle.gz, binary gene x genome table with SparseArray structure
    1) <name>_strain_by_noncoding_allele.csv.gz, binary allele x genome table as flat file
    2) <name>_strain_by_noncoding.csv.gz, binary gene x genome table as flat file
    3) <name>_noncoding_nr.fna, all non-redundant non-coding seqs observed, with headers <name>_T#A#
    4) <name>_noncoding_nr.fna.cdhit.clstr, CD-HIT-EST output file from clustering
    5) <name>_noncoding_allele_names.tsv, mapping between <name>_T#A# to original transcript headers
    6) <name>_noncoding_redundant_headers.tsv, lists of headers sharing the same sequences, with the
        representative header relevant to #5 listed first for each group.
    7) <name>_noncoding_missing_headers.txt, lists headers for original entries missing sequences
    
    Parameters
    ----------
    genome_data : list
        List of 2-tuples (genome_gff, genome_fna) for use by extract_noncoding()
    output_dir : str
        Path to directory to generate outputs and intermediates.
    name : str
        Header to prepend to all output files and allele names (default 'Test')
    flanking : tuple
        (X,Y) where X = number of nts to include from 5' end of feature,
        and Y = number of nts to include from 3' end feature. Features
        may be truncated by contig boundaries (default (0,0))
    allowed_features : list
        List of GFF feature types to extract. Default excludes 
        features labeled "CDS" or "repeat_region" 
        (default ['transcript', 'tRNA', 'rRNA', 'misc_binding'])
    cdhit_args : dict
        Alignment arguments to pass CD-HIT-EST, other than -i, -o, and -d
        (default {'-n':5, '-c':0.8})
    fastasort_path : str
        Path to Exonerate's fastasort binary, optionally for sorting
        final FAA files (default None)
        
    Returns 
    -------
    df_nc_alleles : pd.DataFrame
        Binary non-coding allele x genome table
    df_nc_genes : pd.DataFrame
        Binary non-coding gene x genome table
    '''
    
    ''' Extract non-coding sequences from all genomes '''
    print 'Extracting non-coding sequences...'
    genome_noncoding_paths = []
    for i, gff_fna in enumerate(genome_data):
        ''' Prepare output path '''
        genome_gff, genome_fna = gff_fna
        genome = genome_gff.split('/')[-1][:-4] # trim off path and .gff
        genome_dir = '/'.join(genome_gff.split('/')[:-1]) + '/' if '/' in genome_gff else ''
        genome_nc_dir = genome_dir + 'derived/' # output noncoding sequences here
        if not os.path.exists(genome_nc_dir):
            os.mkdir(genome_nc_dir)
        genome_nc = genome_nc_dir + genome + '_noncoding.fna'
            
        ''' Extract non-coding sequences '''
        print i+1, genome
        genome_noncoding_paths.append(genome_nc)
        extract_noncoding(genome_gff, genome_fna, genome_nc, 
            flanking=flanking, allowed_features=allowed_features)
        
    ''' Reduce to non-redundant sequence set '''
    print 'Identifying non-redundant non-coding sequences...'
    output_nr_fna = output_dir + '/' + name + '_noncoding_nr.fna' # final non-redundant FNA files
    output_shared_headers = output_dir + '/' + name + '_noncoding_redundant_headers.tsv' 
        # records headers that have the same sequence
    output_missing_headers = output_dir + '/' + name + '_noncoding_missing_headers.txt' 
        # records headers without any seqeunce
    output_nr_fna = output_nr_fna.replace('//','/')
    output_shared_headers = output_shared_headers.replace('//','/')
    output_missing_headers = output_missing_headers.replace('//','/')
    nr_seq_hashes, missing_headers = consolidate_seqs(
        genome_noncoding_paths, output_nr_fna, 
        output_shared_headers, output_missing_headers)
    # maps sequence hash to headers of that sequence, in order observed
    
    ''' Apply CD-Hit to non-redundant non-coding sequences '''
    output_nr_fna_copy = output_nr_fna + '.cdhit' # temporary FNA copy generated by CD-HIT-EST
    output_nr_clstr = output_nr_fna + '.cdhit.clstr' # cluster file generated by CD-HIT-EST
    cluster_with_cdhit(output_nr_fna, output_nr_fna_copy, cdhit_args)
    os.remove(output_nr_fna_copy) # delete CD-HIT-EST copied sequences
    
    ''' Extract genes and alleles, rename unique sequences as <name>_T#A# '''
    output_allele_names = output_dir + '/' + name + '_allele_names.tsv' # allele names vs non-redundant headers
    output_allele_names = output_allele_names.replace('//','/')
    header_to_allele = rename_genes_and_alleles(
        output_nr_clstr, output_nr_fna, output_nr_fna, 
        output_allele_names, name=name, cluster_type='noncoding',
        shared_headers_file=output_shared_headers,
        fastasort_path=fastasort_path)
    # maps original headers to short names <name>_T#A#
    
    ''' Process gene/allele membership into binary tables '''    
    df_nr_alleles, df_nr_genes = build_genetic_feature_tables(
        output_nr_clstr, genome_noncoding_paths, name, 
        cluster_type='noncoding', header_to_allele=header_to_allele)
    
    ''' Save tables as PICKLE.GZ (preserve SparseArrays) and CSV.GZ (backup flat file) '''
    output_allele_table = output_dir + '/' + name + '_strain_by_noncoding_allele'
    output_gene_table = output_dir + '/' + name + '_strain_by_noncoding_gene'
    output_allele_table = output_allele_table.replace('//','/')
    output_gene_table = output_gene_table.replace('//','/')
    output_allele_csv = output_allele_table + '.csv.gz'
    output_gene_csv = output_gene_table + '.csv.gz'
    output_allele_pickle = output_allele_table + '.pickle.gz'
    output_gene_pickle = output_gene_table + '.pickle.gz'
    print 'Saving', output_allele_pickle, '...'
    df_nr_alleles.to_pickle(output_allele_pickle)
    print 'Saving', output_allele_csv, '...'
    df_nr_alleles.to_csv(output_allele_csv)
    print 'Saving', output_gene_pickle, '...'
    df_nr_genes.to_pickle(output_gene_pickle)
    print 'Saving', output_gene_csv, '...'
    df_nr_genes.to_csv(output_gene_csv)
    
    return df_nr_alleles, df_nr_genes
    

def consolidate_seqs(genome_paths, nr_out, shared_headers_out, missing_headers_out=None):
    ''' 
    Combines sequences for many genomes into a single file without duplicate
    sequences to be clustered using CD-Hit, i.e. with cluster_with_cdhit(). Tracks
    headers that share the same sequence, and optionally headers without sequences.
    
    Parameters
    ----------
    genome_paths : list
        Paths to genome FAA/FNA files to combine
    nr_out : str
        Output path for combined non-redundant FAA/FNA file
    shared_headers_out : str
        Output path for shared headers TSV file
    missing_headers_out : str
        Output path for headers without sequences TXT file (default None)

    Returns
    -------
    non_redundant_seq_hashes : dict
        Maps non-redundant sequence hashes to a list of headers, in order observed
    missing_headers : list
        List of headers without any associated sequence
    '''
    non_redundant_seq_hashes = {} # maps sequence hash to headers of that sequence, in order observed
    encounter_order = [] # stores sequence hashes in order encountered
    missing_headers = [] # stores headers without sequences
    
    def process_header_and_seq(header, seq_blocks, output_file):
        ''' Processes a header/sequence pair against the running list of non-redundant sequences '''
        seq = ''.join(seq_blocks)
        if len(header) > 0 and len(seq) > 0: # valid header-sequence record
            seqhash = __hash_sequence__(seq)
            if seqhash in non_redundant_seq_hashes: # record repeated appearances of sequence
                non_redundant_seq_hashes[seqhash].append(header)
            else: # first encounter of a sequence, record to non-redundant file
                encounter_order.append(seqhash)
                non_redundant_seq_hashes[seqhash] = [header]
                output_file.write('>' + header + '\n')
                output_file.write('\n'.join(seq_blocks) + '\n')
        elif len(header) > 0 and len(seq) == 0: # header without sequence
            missing_headers.append(header)
    
    ''' Scan for redundant sequences across all files, build non-redundant file '''
    with open(nr_out, 'w+') as f_nr_out:
        for genome_path in genome_paths:
            with open(genome_path, 'r') as f:
                header = ''; seq_blocks = []
                for line in f:
                    if line[0] == '>': # header encountered
                        process_header_and_seq(header, seq_blocks, f_nr_out)
                        header = __get_header_from_fasta_line__(line)
                        seq_blocks = []
                    else: # sequence line encountered
                        seq_blocks.append(line.strip())
                process_header_and_seq(header, seq_blocks, f_nr_out) # process last record
                
    ''' Save shared and missing headers to file '''
    with open(shared_headers_out, 'w+') as f_header_out:
        for seqhash in encounter_order:
            headers = non_redundant_seq_hashes[seqhash]
            if len(headers) > 1:
                f_header_out.write('\t'.join(headers) + '\n')
    if missing_headers_out:
        print 'Headers without sequences:', len(missing_headers)
        with open(missing_headers_out, 'w+') as f_header_out:
            for header in missing_headers:
                f_header_out.write(header + '\n')
                
    return non_redundant_seq_hashes, missing_headers
        
                
def cluster_with_cdhit(fasta_file, cdhit_out, cdhit_args={'-n':5, '-c':0.8}):
    '''
    Runs CD-Hit on a fasta file, i.e. one generated by consolidate_seqs().
    Requires CD-Hit to be available in PATH. Uses CD-HIT for FAA files (default),
    and CD-HIT-EST for FNA files. 
    
    For CD-HIT-EST, word size "-n" has a more direct impact of similarity threshold
    See https://github.com/weizhongli/cdhit/wiki/3.-User's-Guide#CDHITEST
    
    Parameters
    ----------
    fasta_file : str
        Path to FAA/FNA file to be clustered, i.e. from consolidate_seqs()
    cdhit_out : str
        Path to be provided to CD-Hit output argument
    cdhit_args : dict
        Dictionary of alignment arguments to be provided to CD-Hit, other than
        -i, -o, and -d. Default is for FAA files. (default {'-n':5, '-c':0.8})
    ''' 
    cdhit_prog = 'cd-hit-est' if fasta_file[-4:].lower() == '.fna' else 'cd-hit'
    args = [cdhit_prog, '-i', fasta_file, '-o', cdhit_out, '-d', '0']
    for arg in cdhit_args:
        args += [arg, str(cdhit_args[arg])]
    print 'Running:', args
    for line in __stream_stdout__(' '.join(args)):
        print line

        
def rename_genes_and_alleles(clstr_file, nr_fasta_in, nr_fasta_out, 
                             feature_names_out, name='Test', cluster_type='cds',
                             shared_headers_file=None, fastasort_path=None):
    '''
    Processes a CD-Hit CLSTR file (clstr_file) to rename headers in the orignal
    fasta as <name>_C#A# for CDS, or <name>_T#A# for non-coding features,
    based on cluster membership and stores header-name mappings as a TSV.
    
    Can optionally sort final fasta file if fastasort_path is specified, from Exonerate
    https://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate
    
    Parameters
    ----------
    clstr_file : str
        Path to CLSTR file generated by CD-Hit
    nr_fasta_in : str
        Path to FAA/FNA file corresponding to clstr_file
    nr_fasta_out : str
        Output path for renamed FAA/FNA, will overwrite if equal to nr_fasta_in
    feature_names_out : str
        Output path for header-allele name mapping TSV file
    name : str
        Header to append output files and allele names (default 'Test')
    cluster_type : str
        If 'cds', features are named <name>_C#A# for gene clusters/alleles. 
        If 'noncoding', features are named <name>_T#A# for transcripts (default 'cds')
    shared_headers_file : str
        Path to shared headers. If provided, will expand the header-allele
        mapping to include headers that map to the same sequence/allele (default None)
    fastasort_path : str
        Path to Exonerate fastasort, used to optionally sort nr_faa (default None)
        
    Returns
    -------
    header_to_allele : dict
        Maps original headers to new allele names
    '''
    
    ''' Optionally, load up shared headers '''
    shared_headers = {} # maps representative header to synonym headers
    if shared_headers_file:
        with open(shared_headers_file, 'r') as f_share:
            for line in f_share:
                headers = line.strip().split('\t')
                representative_header = headers[0]
                synonym_headers = headers[1:]
                shared_headers[representative_header] = synonym_headers
    
    ''' Read through CLSTR file to map original headers to C#A#/T#A# names '''
    header_to_allele = {} # maps headers to allele name (name_C#A# or name_T#A#)
    max_cluster = 0 
    with open(feature_names_out, 'w+') as f_naming:
        with open(clstr_file, 'r') as f_clstr:
            for line in f_clstr:
                if line[0] == '>': # starting new gene cluster
                    cluster_num = line.split()[-1].strip() # cluster number as string
                    max_cluster = cluster_num
                else: # adding allele to cluster
                    data = line.split()
                    allele_num = data[0] # allele number as string
                    allele_header = data[2][1:-3] # old allele header
                    allele_name = create_feature_name(name, cluster_type, cluster_num, 'allele', allele_num)
                    header_to_allele[allele_header] = allele_name
                    mapped_headers = [allele_header]
                    if allele_header in shared_headers: # if synonym headers are available
                        for synonym_header in shared_headers[allele_header]:
                            header_to_allele[synonym_header] = allele_name
                        mapped_headers += shared_headers[allele_header]
                    f_naming.write(allele_name + '\t' + ('\t'.join(mapped_headers)).strip() + '\n')
                    
    ''' Create the fasta file with renamed features '''
    with open(nr_fasta_in, 'r') as f_fasta_old:
        with open(nr_fasta_out + '.tmp', 'w+') as f_fasta_new:
            ''' Iterate through alleles in cluster/allele order '''
            for line in f_fasta_old:
                if line[0] == '>': # writing updated header line
                    allele_header = line[1:].strip()
                    if allele_header in header_to_allele:
                        allele_name = header_to_allele[allele_header]
                    else:
                        print 'MISSING:', allele_header
                    f_fasta_new.write('>' + allele_name + '\n')
                else: # writing sequence line
                    f_fasta_new.write(line)
    
    ''' Move fasta file to desired output path '''
    if nr_fasta_out == nr_fasta_in: # if overwriting, remove old faa file
        os.remove(nr_fasta_in) 
    os.rename(nr_fasta_out + '.tmp', nr_fasta_out)
    
    ''' If available, use exonerate.fastasort to sort entries in fasta file '''
    if fastasort_path:
        print 'Sorting sequences by header...'
        args = ['./' + fastasort_path, nr_fasta_out]
        with open(nr_fasta_out + '.tmp', 'w+') as f_sort:
            sp.call(args, stdout=f_sort)
        os.rename(nr_fasta_out + '.tmp', nr_fasta_out)
    return header_to_allele
    

def build_genetic_feature_tables(clstr_file, genome_fasta_paths, name='Test', cluster_type='cds',
                                 shared_header_file=None, header_to_allele=None):
    '''
    Builds two binary tables based on the presence/absence of genetic features, 
    allele x genome (allele_table_out) and gene x genome (gene_table_out).
    Works for both CDS clusters and non-codnig transcript clusters.
    Uses a CD-Hit CLSTR file, the corresponding original fasta files, 
    and shared header mappings.
    
    Parameters
    ----------
    clstr_file : str
        Path to CD-Hit CLSTR file used to build header-allele mappings
    genome_fasta_paths : list
        Paths to genome fasta files originally combined and clustered (see consolidate_seqs)
    name : str
        Name to attach to features and files (default 'Test')
    cluster_type : str
        If 'cds', features are named <name>_C#A# for gene clusters/alleles. 
        If 'noncoding', features are named <name>_T#A# for transcripts (default 'cds')
    shared_header_file : str
        Path to shared header TSV file, if synonym headers are not mapped
        in header_to_allele or header_to_allele is not provided (default None)
    header_to_allele : dict
        Pre-calculated header-allele mappings corresponding to clstr_file,
        if available from rename_genes_and_alleles() (default None)

    Returns 
    -------
    df_alleles : pd.DataFrame
        Binary allele x genome table
    df_genes : pd.DataFrame
        Binary gene x genome table
    '''
    
    ''' Load header-allele mappings '''
    print 'Loadings header-allele mappings...'
    header_to_allele = load_header_to_allele(clstr_file, 
        shared_header_file, header_to_allele, cluster_type)
                    
    ''' Initialize gene and allele tables '''
    fasta_to_genome = lambda x: x.split('/')[-1][:-4] # removes folders and .fna/.faa from end
    genome_order = sorted([fasta_to_genome(x) for x in genome_fasta_paths]) # for genome names, trim .faa from filenames
    print 'Sorting alleles...'
    allele_order = sorted(list(set(header_to_allele.values())))
    
    print 'Sorting clusters...'
    gene_order = []; last_gene = None
    for allele in allele_order:
        gene = __get_gene_from_allele__(allele)
        if gene != last_gene:
            gene_order.append(gene)
            last_gene = gene
    print 'Genomes:', len(genome_order)
    print 'Clusters:', len(gene_order)
    print 'Alleles:', len(allele_order)
    
    ''' To use sparse matrix, map genomes, alleles, and genes to positions '''
    allele_indices = {allele_order[i]:i for i in range(len(allele_order))}
    gene_indices = {gene_order[i]:i for i in range(len(gene_order))}   
    allele_arrays = {} # maps genome:allele vectors as SparseArrays
    gene_arrays = {} # maps genome:gene vector as SparseArrays

    ''' Scan original genome file for allele and gene membership '''
    for i, genome_fasta in enumerate(sorted(genome_fasta_paths)):
        genome = fasta_to_genome(genome_fasta)
        genome_i = genome_order.index(genome)
        allele_arrays[genome] = np.zeros(shape=len(allele_order), dtype='int64')
        gene_arrays[genome] = np.zeros(shape=len(gene_order), dtype='int64')
        with open(genome_fasta, 'r') as f_fasta:
            header = ''; seq = '' # track the sequence to skip over empty sequences
            for line in f_fasta.readlines(): # pre-load, slight speed-up
                ''' Load all alleles and genes per genome '''
                if line[0] == '>': # new header line encountered
                    if len(seq) > 0:
                        if header in header_to_allele:
                            allele_name = header_to_allele[header]
                            allele_i = allele_indices[allele_name]
                            allele_arrays[genome][allele_i] = 1
                            gene = __get_gene_from_allele__(allele_name)
                            gene_i = gene_indices[gene]
                            gene_arrays[genome][gene_i] = 1
                        else:
                            print 'MISSING:', allele_header
                    header = __get_header_from_fasta_line__(line)
                    seq = '' # reset sequence
                else: # sequence line encountered
                    seq += line.strip()
            if len(seq) > 0: # process last record
                if header in header_to_allele:
                    allele_name = header_to_allele[header]
                    allele_i = allele_indices[allele_name]
                    allele_arrays[genome][allele_i] = 1
                    gene = __get_gene_from_allele__(allele_name)
                    gene_i = gene_indices[gene]
                    gene_arrays[genome][gene_i] = 1
                else:
                    print 'MISSING:', header
                
        allele_arrays[genome] = pd.SparseArray(allele_arrays[genome])
        gene_arrays[genome] = pd.SparseArray(gene_arrays[genome])
        allele_arrays[genome].fill_value = np.nan
        gene_arrays[genome].fill_value = np.nan
        print 'Updating genome', i+1, ':', genome, 
        print '\tAlleles:', allele_arrays[genome].sum(), '\tClusters:', gene_arrays[genome].sum()
        
    ''' Construct DataFrame '''
    print 'Building DataFrame...'
    df_alleles = pd.DataFrame(data=allele_arrays, index=allele_order)
    df_genes = pd.DataFrame(data=gene_arrays, index=gene_order)
    return df_alleles, df_genes


def load_header_to_allele(clstr_file=None, shared_header_file=None, 
                          header_to_allele=None, name='Test', cluster_type='cds'):
    '''
    Loads a mapping from original fasta headers to allele names format 
    <name>_C#A# for genes or <name>_T#A# for noncoding features.
    
    Parameters
    ----------
    clstr_file : str
        Path to CD-Hit CLSTR file used to build header-allele mappings,
        only needed if header_to_allele is None (default None)
    shared_header_file : str
        Path to shared header TSV file, if synonym headers are not mapped
        in header_to_allele or header_to_allele is not provided (default None)
    header_to_allele : dict
        Pre-calculated header-allele mappings corresponding to clstr_file,
        if available from rename_genes_and_alleles (default None)
    name : str
        Name to attach to features and files (default 'Test')
    cluster_type : str
        If 'cds', features are named <name>_C#A# for gene clusters/alleles. 
        If 'noncoding', features are named <name>_T#A# for transcripts (default 'cds')
        
    Returns
    -------
    full_header_to_allele : dict
        Full header-allele mappings combining contents of both header_to_allele 
        (copied or built from clstr_file) and shared_header_file.
    '''
    
    ''' Load header to allele mapping from CLSTR, if not provided '''
    if header_to_allele is None:
        full_header_to_allele = {} # maps representative header to allele name (name_C#A#)
        with open(clstr_file, 'r') as f_clstr:
            for line in f_clstr:
                if line[0] == '>': # starting new gene cluster
                    cluster_num = line.split()[-1].strip() # cluster number as string
                    max_cluster = cluster_num
                else: # adding allele to cluster
                    data = line.split()
                    allele_num = data[0] # allele number as string
                    allele_header = data[2][1:-3] # old allele header
                    allele_name = create_feature_name(name, cluster_type, cluster_num, 'allele', allele_num)
                    full_header_to_allele[allele_header] = allele_name
    elif type(header_to_allele) == dict:
        full_header_to_allele = header_to_allele.copy()
    
    ''' Load headers that share the same sequence '''
    if shared_header_file:
        with open(shared_header_file, 'r') as f_header:
            for line in f_header:
                headers = [x.strip() for x in line.split('\t')]
                if len(headers) > 1:
                    repr_header = headers[0]
                    repr_allele = full_header_to_allele[repr_header]
                    for alt_header in headers[1:]:
                        full_header_to_allele[alt_header] = repr_allele
    return full_header_to_allele


def build_upstream_pangenome(genome_data, allele_names, output_dir, limits=(-50,3), name='Test', 
                             include_fragments=False, max_overlap=-1, fastasort_path=None):
    '''
    Extracts nucleotides upstream of coding sequences for multiple genomes, 
    create <genome>_upstream.fna files in the same directory for each genome.
    Then, classifies/names them relative to gene clusters identified by coding sequence,  
    i.e. after build_cds_pangenome(). See build_proximal_pangenome() for parameters.
    '''
    return build_proximal_pangenome(
        genome_data, allele_names, output_dir, limits, 
        side='upstream', name=name, include_fragments=include_fragments, 
        max_overlap=max_overlap, fastasort_path=fastasort_path)
    

def build_downstream_pangenome(genome_data, allele_names, output_dir, limits=(-3,50), name='Test', 
                               include_fragments=False, max_overlap=-1, fastasort_path=None):
    '''
    Extracts nucleotides downstream of coding sequences for multiple genomes, 
    create <genome>_downstream.fna files in the same directory for each genome.
    Then, classifies/names them relative to gene clusters identified by coding sequence,  
    i.e. after build_cds_pangenome(). See build_proximal_pangenome() for parameters.
    '''
    return build_proximal_pangenome(
        genome_data, allele_names, output_dir, limits, 
        side='downstream', name=name, include_fragments=include_fragments, 
        max_overlap=max_overlap, fastasort_path=fastasort_path)

    
def build_proximal_pangenome(genome_data, allele_names, output_dir, limits, side, name='Test', 
                             include_fragments=False, max_overlap=-1, fastasort_path=None):
    '''
    Extracts nucleotides proximal to coding sequences for multiple genomes, 
    create genome-specific proximal sequence fna files in the same directory for each genome.
    Then, classifies/names them relative to gene clusters identified by coding sequence,  
    i.e. after build_cds_pangenome(). See extract_proximal_sequences() and
    consolidate_proximal() for more details.
    
    Parameters
    ----------
    genome_data : list
        List of 2-tuples (genome_gff, genome_fna) for use by extract_proximal_sequences()
    allele_names : str
        Path to allele names file, should be named <name>_allele_names.tsv
    output_dir : str
        Path to directory to generate summary outputs.
    limits : 2-tuple
        Length of proximal region to extract, formatted (-X,Y). For upstream, extracts X 
        upstream bases (up to but excluding first base of start codon) and first Y coding 
        bases (including first base of start codon). For downstream, extracts the last X
        coding bases and Y downstream bases. In both cases, the total length is X+Y bases.
    side : str
        'upstream' or 'downstream' for 5'UTR or 3'UTR
    name : str
        Short header to prepend output summary files, recommendated to be same as what
        was used in the build_cds_pangenome() (default 'Test')
    include_fragments : bool
        If true, include proximal sequences that are not fully available 
        due to contig boundaries (default False)
    max_overlap : int
        If non-negative, truncates UTRs that cross over into coding sequences
        of other genes such that the overlap is no more than <max_overlap> nts.
        If negative, does not truncate UTRs s.t. all UTRs same length (default -1)
    fastasort_path : str
        Path to Exonerate's fastasort binary, optionally for sorting
        final FNA files (default None)
        
    Returns
    -------
    df_proximal : pd.DataFrame
        Binary proximal x genome table
    '''
    
    ''' Load header-allele name mapping '''
    print 'Loading header-allele mapping...'
    feature_to_allele = __load_feature_to_allele__(allele_names)
        
    ''' Generate proximal sequences '''
    print 'Extracting', side, 'sequences...'
    genome_proximals = []
    for i, gff_fna in enumerate(genome_data):
        ''' Prepare output path '''
        genome_gff, genome_fna = gff_fna
        genome = genome_gff.split('/')[-1][:-4] # trim off path and .gff
        genome_dir = '/'.join(genome_gff.split('/')[:-1]) + '/' if '/' in genome_gff else ''
        genome_prox_dir = genome_dir + 'derived/'
        if not os.path.exists(genome_prox_dir):
            os.mkdir(genome_prox_dir)
        genome_prox = genome_prox_dir + genome + '_' + side + '.fna'
            
        ''' Extract proximal sequences '''
        print i+1, genome
        genome_proximals.append(genome_prox)
        extract_proximal_sequences(genome_gff, genome_fna, genome_prox, 
                                   limits=limits, side=side,
                                   feature_to_allele=feature_to_allele,
                                   include_fragments=include_fragments,
                                   max_overlap=max_overlap)
        
    ''' Consolidate non-redundant proximal sequences per gene '''
    print 'Identifying non-redundant', side, 'sequences per gene...'
    nr_prox_out = output_dir + '/' + name + '_nr_' + side + '.fna'
    nr_prox_out = nr_prox_out.replace('//','/')
    df_proximal = consolidate_proximal(genome_proximals, nr_prox_out, feature_to_allele, side)
    
    ''' Optionally sort non-redundant proximal sequences file '''
    if fastasort_path:
        print 'Sorting sequences by header...'
        args = ['./' + fastasort_path, nr_prox_out]
        with open(nr_prox_out + '.tmp', 'w+') as f_sort:
            sp.call(args, stdout=f_sort)
        os.rename(nr_prox_out + '.tmp', nr_prox_out)
        
    ''' Save proximal x genome table '''
    prox_table_out = output_dir + '/' + name + '_strain_by_' + side
    prox_table_out = prox_table_out.replace('//','/')
    prox_table_pickle = prox_table_out + '.pickle.gz'
    prox_table_csv = prox_table_out + '.csv.gz'
    print 'Saving', prox_table_pickle, '...'
    df_proximal.to_pickle(prox_table_pickle)
    print 'Saving', prox_table_csv, '...'
    df_proximal.to_csv(prox_table_csv)
    return df_proximal

    
def consolidate_proximal(genome_proximals, nr_proximal_out, feature_to_allele, side):
    ''' 
    Consolidates proximal sequences to a non-redudnant set with respect to each
    gene described by feature_to_allele (maps_features to <name>_C#A#), then
    creates a proximal x genome binary table. For use with fixed-length 3' or 5'UTRs.
    Upstream features are <name>_C#U#, downstream features are <name>_C#D#.
    
    Parameters
    ----------
    genome_proximals : list
        List of paths to proximal sequences FNA to combine
    nr_proximal_out : str
        Path to output non-redundant proximal sequences as FNA
    feature_to_allele : dict
        Dictionary mapping headers to <name>_C#A# alleles
    side : str
        'upstream' or 'downstream' for 5'UTR or 3'UTR
    
    Returns
    -------
    df_proximal : pd.DataFrame
         Binary proximal x genome table
    '''
    
    ftype_abb = VARIANT_TYPES[side]
    gene_to_unique_proximal = {} # maps gene:prox_seq:prox_seq_id (int)
    genome_to_proximal = {} # maps genome:proximal_name:1 if present (<name>_C#U# or <name>_C#D#)
    unique_proximal_ids = set() # record non-redundant list of proximal sequence IDs <name>_C#U# or <name>_C#D#
    genome_order = [] # sorted list of genomes inferred from proximal sequence file names
    
    with open(nr_proximal_out, 'w+') as f_nr_prox:
        for genome_proximal in sorted(genome_proximals):
            ''' Infer genome name from genome filename '''
            genome = genome_proximal.split('/')[-1] # trim off full path
            genome = genome.split('_' + side)[0] # remove .fna footer
            genome_to_proximal[genome] = {}
            genome_order.append(genome)
            
            ''' Process genome's proximal record '''
            with open(genome_proximal, 'r') as f_prox: # reading current proximal seq file
                header = ''; prox_seq = ''; new_sequence = False
                for line in f_prox.readlines(): # slight speed up reading whole file at once, should only be few MBs
                    if line[0] == '>': # header line
                        if len(prox_seq) > 0:
                            ''' Process header-seq to non-redundant <name>_C#<U/D># proximal allele '''
                            feature = header.split('_' + side + '(')[0] # trim off "_<up/down>stream" footer
                            allele = feature_to_allele[feature] # get <name>_C#A# allele
                            gene = __get_gene_from_allele__(allele) # gene <name>_C# gene
                            if not gene in gene_to_unique_proximal:
                                gene_to_unique_proximal[gene] = {}
                            if not prox_seq in gene_to_unique_proximal[gene]:
                                gene_to_unique_proximal[gene][prox_seq] = len(gene_to_unique_proximal[gene])
                                prox_id = gene + ftype_abb + str(gene_to_unique_proximal[gene][prox_seq])
                                unique_proximal_ids.add(prox_id)
                                new_sequence = True
                            prox_id = gene + ftype_abb + str(gene_to_unique_proximal[gene][prox_seq])
                            genome_to_proximal[genome][prox_id] = 1
                            
                            ''' Write renamed sequence to running file '''
                            if new_sequence:
                                f_nr_prox.write('>' + prox_id + '\n')
                                f_nr_prox.write(prox_seq + '\n')
                                new_sequence = False

                        header = line[1:].strip(); prox_seq = ''
                    else: # sequence line
                        prox_seq += line.strip()
            
                ''' Process last record'''
                feature = header.split('_' + side + '(')[0] # trim off "_<up/down>stream" footer
                allele = feature_to_allele[feature] # get <name>_C#A# allele
                gene = __get_gene_from_allele__(allele) # gene <name>_C# gene
                if not gene in gene_to_unique_proximal:
                    gene_to_unique_proximal[gene] = {}
                if not prox_seq in gene_to_unique_proximal[gene]:
                    gene_to_unique_proximal[gene][prox_seq] = len(gene_to_unique_proximal[gene])
                    prox_id = gene + ftype_abb + str(gene_to_unique_proximal[gene][prox_seq])
                    unique_proximal_ids.add(prox_id)
                    new_sequence = True
                prox_id = gene + ftype_abb + str(gene_to_unique_proximal[gene][prox_seq])
                genome_to_proximal[genome][prox_id] = 1

                ''' Write renamed sequence to running file '''
                if new_sequence:
                    f_nr_prox.write('>' + prox_id + '\n')
                    f_nr_prox.write(prox_seq + '\n')
                    new_sequence = False
                    
    ''' Convert nested dict to dict of genome:SparseArrays once all proximal sequences are known '''
    print 'Sparsifying', side, 'table...'
    prox_order = sorted(list(unique_proximal_ids))
    del unique_proximal_ids
    prox_indices = {prox_order[i]:i for i in range(len(prox_order))} # map proximal ID to index
    for g,genome in enumerate(genome_order):
        prox_array = np.zeros(shape=len(prox_order), dtype='int64')
        for genome_prox in genome_to_proximal[genome].keys():
            prox_i = prox_indices[genome_prox]
            prox_array[prox_i] = 1
        genome_to_proximal[genome] = pd.SparseArray(prox_array)
        genome_to_proximal[genome].fill_value = np.nan
        
    print 'Constructing DataFrame...'
    df_proximal = pd.DataFrame(data=genome_to_proximal, index=prox_order)
    return df_proximal


def extract_upstream_sequences(genome_gff, genome_fna, upstream_out, limits=(-50,3), max_overlap=-1,
                               feature_to_allele=None, allele_names=None, include_fragments=False):
    '''
    Extracts nucleotides upstream of coding sequences to file, default 50bp + start codon.
    Refer to extract_proximal_sequences() for parameters.
    '''
    extract_proximal_sequences(genome_gff, genome_fna, proximal_out=upstream_out, 
                               limits=limits, max_overlap=max_overlap, side='upstream', 
                               feature_to_allele=feature_to_allele, allele_names=allele_names,
                               include_fragments=include_fragments)

    
def extract_downstream_sequences(genome_gff, genome_fna, downstream_out, limits=(-3,50), max_overlap=-1,
                                 feature_to_allele=None, allele_names=None, include_fragments=False):
    '''
    Extracts nucleotides downstream of coding sequences to file, default stop codon + 50 bp.
    Refer to extract_proximal_sequences() for parameters.
    '''
    extract_proximal_sequences(genome_gff, genome_fna, proximal_out=downstream_out, 
                               limits=limits, max_overlap=max_overlap, side='downstream', 
                               feature_to_allele=feature_to_allele, allele_names=allele_names,
                               include_fragments=include_fragments)
    
                
def extract_proximal_sequences(genome_gff, genome_fna, proximal_out, limits, max_overlap, side,
                               feature_to_allele=None, allele_names=None, include_fragments=False):
    '''
    Extracts nucleotides upstream or downstream of coding sequences. 
    Interprets GFFs as formatted by PATRIC:
        1) Assumes contigs are labeled "accn|<contig>". 
        2) Assumes protein features have ".peg." in the ID
        3) Assumes ID = fig|<genome>.peg.#
    Output features are named "<feature header>_<up/down>stream(<limit1>,<limit2>)" 
    if overlap is not restricted, otherwise features are named:
    "<feature header>_<up/down>stream(<limit1>,<limit2>,<max_overlap>)"
    Excludes features that do not have any UTR bases.
        
    Parameters
    ----------
    genome_gff : str
        Path to genome GFF file with CDS coordinates
    genome_fna : str
        Path to genome FNA file with contig nucleotides
    proximal_out : str
        Path to output upstream/downstream sequences FNA files
    limits : 2-tuple
        Length of proximal region to extract, formatted (-X,Y). For upstream, extracts X 
        upstream bases (up to but excluding first base of start codon) and first Y coding 
        bases (including first base of start codon). For downstream, extracts the last X
        coding bases and Y downstream bases. In both cases, the total length is X+Y bases.
    max_overlap : int
        If non-negative, truncates UTRs that cross over into coding sequences
        of other genes such that the overlap is no more than <max_overlap> nts.
        If negative, does not truncate UTRs s.t. all UTRs same length. 
        Note: Requires that GFF has features sorted by position.
    side : str
        'upstream' or 'downstream' for 5'UTR or 3'UTR
    feature_to_allele : dict, str
        Dictionary mapping original feature headers to <name>_C#A# short names,
        alternatively, the allele_names file can be provided (default None)
    allele_names : str
        Path to allele names file if feature_to_allele is not provided,
        should be named <name>_allele_names.tsv. If neither are provided,
        simply processes all features present in the GFF (default None)
    include_fragments : bool
        If true, include upstream sequences that are not fully available 
        due to contig boundaries (default False)
    '''

    def extract_utr(start, stop, strand, contig, contig_seq, occupancy):
        ''' Calculate ideal bounds for UTR, without accounting for overlap '''
        pos = (side, strand)
        utr_side = start if pos in [('upstream','+'),('downstream','-')] else stop # side UTR effectively starts from
        utr_limits = limits if strand == '+' else (-limits[1], -limits[0]) # how to extend UTR bounds
        utr_start = utr_side + utr_limits[0]
        utr_stop = utr_side + utr_limits[1]
        
        ''' Optionally account for overlap '''
        if max_overlap >= 0: # checking CDS-UTR overlaps
            leftbound, rightbound = strand_occupancy[contig][strand][(start, stop)]
            leftbound -= max_overlap
            rightbound += max_overlap
            if utr_start < leftbound: # 5' overlap exceeds limit
                utr_start = leftbound
                #print 'UTR start overlap for', start, stop
            if utr_stop > rightbound: # 3' overlap exceeds limit
                utr_stop = rightbound
                #print 'UTR stop overlap for', start, stop
            
        ''' Extract UTR from computed bounds, RC if negative strand '''
        proximal = contig_seq[utr_start:utr_stop].strip()
        proximal = reverse_complement(proximal) if strand == '-' else proximal
        is_fragment = (utr_start < 0) or (utr_stop > len(contig_seq)) # if cut-off by contig bounds
        return proximal, is_fragment
    
    
    strand_occupancy = {} # maps contig:strand:(start,stop):[(left start,stop), (right start,stop)]
    if max_overlap >= 0:   
        ''' Load feature order on each contig and each strand '''
        occupancies = {}
        with open(genome_gff, 'r') as f_gff:
            for line in f_gff:
                line = line.strip()
                if len(line) > 0 and line[0] != '#':
                    contig, src, feat_type, start, stop, score, \
                        strand, phase, attr_raw = line.split('\t')
                    if feat_type == 'CDS': # only consider CDS-UTR overlaps
                        contig = contig.split('|')[-1] # accn|<contig> to just <contig>
                        if not contig in occupancies:
                            occupancies[contig] = {'+':[], '-':[]}
                        start = int(start) - 1 # starts are 1-indexed
                        stop = int(stop) # stops 1-indexed, inclusive = correct exclusive bound
                        occupancies[contig][strand].append( (start, stop) )
                         
        ''' Convert feature order to 5'/3' neighbors pairs '''
        for contig in occupancies:
            strand_occupancy[contig] = {'+':{}, '-':{}}
            for strand in occupancies[contig]:
                n_features = len(occupancies[contig][strand])
                for i, feature in enumerate(occupancies[contig][strand]): # features in order on strand
                    leftbound = (-np.inf,-np.inf) if i == 0 else occupancies[contig][strand][i-1]
                    rightbound = (np.inf,np.inf) if i == n_features-1 else occupancies[contig][strand][i+1]
                    leftbound = leftbound[1] # take 3' end of nearest CDS to the 5' side
                    rightbound = rightbound[0] # take 5' end of nearest CDS to the 3' end
                    strand_occupancy[contig][strand][feature] = (leftbound, rightbound) 
        del occupancies
                        
    ''' Load contig sequences '''
    contigs = load_sequences_from_fasta(genome_fna, header_fxn=lambda x: x.split()[0])
            
    ''' Load header-allele name mapping '''
    if feature_to_allele: # dictionary provided directly
        feat_to_allele = feature_to_allele
    elif allele_names: # allele map file provided
        feat_to_allele = __load_feature_to_allele__(allele_names)
    else: # no allele mapping, process everything
        feat_to_allele = None
                    
    ''' Parse GFF file for CDS coordinates '''
    feature_footer = '_' + side
    params = (limits[0], limits[1], max_overlap) if max_overlap > 0 else limits
    feature_footer += str(params).replace(' ','')
    proximal_count = 0 # total UTRs extracted
    coding_length = limits[1] if side == 'upstream' else -limits[0] # bases of UTR that overlap with reference gene CDS
    with open(proximal_out, 'w+') as f_prox:
        with open(genome_gff, 'r') as f_gff:
            for line in f_gff:
                line = line.strip()
                if len(line) > 0 and line[0] != '#':
                    contig, src, feat_type, start, stop, score, \
                        strand, phase, attr_raw = line.split('\t')
                    contig = contig.split('|')[-1] # accn|<contig> to just <contig>
                    start = int(start) - 1 # starts are 1-indexed, inclusive
                    stop = int(stop) # stops 1-indexed, inclusive = correct exclusive bound
                    attrs = {} # key:value
                    for entry in attr_raw.split(';'):
                        k,v = entry.split('='); attrs[k] = v
                    gffid = attrs['ID']

                    ''' Verify allele has been mapped, and contig has been identified '''
                    if contig in contigs: 
                        if gffid in feat_to_allele or feat_to_allele is None:
                            contig_seq = contigs[contig]
                            proximal, is_fragment = extract_utr(start, stop, strand, contig, contig_seq, strand_occupancy)
                                
                            ''' Save proximal sequence '''
                            if len(proximal) > coding_length and (not is_fragment or include_fragments):
                                feat_name = gffid + feature_footer
                                f_prox.write('>' + feat_name + '\n')
                                f_prox.write(proximal + '\n')
                                proximal_count += 1
                                
    print 'Loaded', side, 'sequences:', proximal_count

    
def extract_noncoding(genome_gff, genome_fna, noncoding_out, flanking=(0,0),
                      allowed_features=['transcript', 'tRNA', 'rRNA', 'misc_binding']):
    '''
    Extracts nucleotides for non-coding sequences. 
    Interprets GFFs as formatted by PATRIC:
        1) Assumes contigs are labeled "accn|<contig>". 
        2) Assumes protein features have ".peg." in the ID
        3) Assumes ID = fig|<genome>.peg.#
        
    Parameters
    ----------
    genome_gff : str
        Path to genome GFF file with CDS coordinates
    genome_fna : str
        Path to genome FNA file with contig nucleotides
    noncoding_out : str
        Path to output transcript sequences FNA files
    flanking : tuple
        (X,Y) where X = number of nts to include from 5' end of feature,
        and Y = number of nts to include from 3' end feature. Features
        may be truncated by contig boundaries (default (0,0))
    allowed_features : list
        List of GFF feature types to extract. Default excludes 
        features labeled "CDS" or "repeat_region" 
        (default ['transcript', 'tRNA', 'rRNA', 'misc_binding'])
    '''
    contigs = load_sequences_from_fasta(genome_fna, header_fxn=lambda x:x.split()[0])
    with open(noncoding_out, 'w+') as f_noncoding:
        with open(genome_gff, 'r') as f_gff:
            for line in f_gff:
                ''' Check for non-comment and non-empty line '''
                if not line[0] == '#' and not len(line.strip()) == 0: 
                    contig, src, feature_type, start, stop, \
                        score, strand, phase, meta = line.split('\t')
                    contig = contig[5:] # trim off "accn|" header
                    start = int(start)
                    stop = int(stop)
                    
                    if feature_type in allowed_features: 
                        ''' Get noncoding feature sequence and ID '''
                        contig_seq = contigs[contig]
                        fstart = start - 1 - flanking[0]
                        fstart = max(0,fstart) # avoid looping due to contig boundaries
                        fstop = stop + flanking[1]
                        feature_seq = contig_seq[fstart:fstop]
                        if strand == '-': # negative strand
                            feature_seq = reverse_complement(feature_seq)
                        meta_key_vals = map(lambda x: x.split('='), meta.split(';'))
                        metadata = {x[0]:x[1] for x in meta_key_vals}
                        feature_id = metadata['ID']
                        
                        ''' Save to output file '''
                        feature_seq = '\n'.join(feature_seq[i:i+70] for i in range(0, len(feature_seq), 70))
                        f_noncoding.write('>' + feature_id + '\n')
                        f_noncoding.write(feature_seq + '\n') 

    
def validate_gene_table(df_genes, df_alleles):
    '''
    Verifies that the gene x genome table is consistent with the
    corresponding allele x genome table. Optimized to run column-by-column
    rather than gene-by-gene for sparse tables.
    
    Parameters
    ----------
    df_genes : pd.DataFrame or str
        Either the gene x genome table, or path to the table
    df_alleles : pd.DataFrame or str
        Either the allele x genome table, or path to the table
    '''
    dfg = load_feature_table(df_genes)
    dfa = load_feature_table(df_alleles)
    print 'Validating gene clusters...'
    num_inconsistencies = 0
    for g,genome in enumerate(df_genes.columns):
        print g+1, 'Testing', genome
        genes = set(dfg[genome].dropna().index)
        alleles = dfa[genome].dropna().index
        allele_genes = set(alleles.map(__get_gene_from_allele__))
        if genes != allele_genes:
            inconsistencies = genes.symmetric_difference(allele_genes)
            print '\tInconsistent:', inconsistencies
            num_inconsistencies += len(inconsistencies)
    
    print 'Total Inconsistencies:', num_inconsistencies 


def validate_gene_table_dense(df_genes, df_alleles):
    '''
    Verifies that the gene x genome table is consistent with the
    corresponding allele x genome table. Original approach for
    when df_genes and df_alleles are dense DataFrames.
    
    Parameters
    ----------
    df_genes : pd.DataFrame or str
        Either the gene x genome table, or path to the table
    df_alleles : pd.DataFrame or str
        Either the allele x genome table, or path to the table
    '''
    dfg = load_feature_table(df_genes)
    dfa = load_feature_table(df_alleles)
    print 'Validating gene clusters...'
    
    current_cluster = None; allele_data = []; 
    clusters_tested = 0; inconsistencies = 0
    for allele_row in dfa.fillna(0).itertuples(name=None):
        cluster = __get_gene_from_allele__(allele_row[0])
        if current_cluster is None: # initializing
            current_cluster = cluster
        elif current_cluster != cluster: # end of gene cluster
            alleles_all = np.array(allele_data)
            has_gene = alleles_all.sum(axis=0) > 0
            is_consistent = np.array_equal(has_gene, dfg.loc[current_cluster,:].fillna(0).values)
            clusters_tested += 1
            if not is_consistent:
                print 'Inconsistent', cluster
                print has_gene
                print dfg.loc[current_cluster,:].fillna(0).values
                inconsistencies += 1
            if clusters_tested % 1000 == 0:
                print '\tTested', clusters_tested, 'clusters'
            allele_data = []
            current_cluster = cluster
        allele_data.append(np.array(allele_row[1:]))
    
    ''' Process final line '''
    alleles_all = np.array(allele_data) 
    has_gene = alleles_all.sum(axis=0) > 0
    is_consistent = np.array_equal(has_gene, dfg.loc[current_cluster,:].fillna(0).values)
    if not is_consistent:
        print 'Inconsistent', cluster
        print has_gene
        print dfg.loc[current_cluster,:].fillna(0).values
        inconsistencies += 1
    print 'Inconsistencies:', inconsistencies


def validate_allele_table(df_alleles, genome_faa_paths, alleles_faa):
    '''
    Verifies that the allele x genome table is consistent with the original FAA files.
    
    Parameters
    ----------
    df_alleles : pd.DataFrame or str
        Either the allele x genome table, or path to the table as CSV or CSV.GZ
    genome_faa_paths : list
        Paths to genome FAA files originally combined and clustered
    alleles_faa : str
        Path to non-redundant sequences corresponding to df_alleles
    '''
    dfa = load_feature_table(df_alleles)
    inconsistencies = 0
    
    ''' Pre-load hashes for non-redundant protein sequences '''
    print 'Loading non-redundant sequences...'
    seqhash_to_allele = {}
    with open(alleles_faa, 'r') as f_faa:
        header = ''; seq_blocks = []
        for line in f_faa:
            if line[0] == '>': # new sequence encountered
                if len(seq_blocks) > 0:
                    seqhash = __hash_sequence__(''.join(seq_blocks))
                    seqhash_to_allele[seqhash] = header
                header = line[1:].strip()
                seq_blocks = []
            else: # sequence encountered
                seq_blocks.append(line.strip())
        # process last record
        seqhash = __hash_sequence__(''.join(seq_blocks))
        seqhash_to_allele[seqhash] = header
                        
    ''' Validate individual genomes against table '''
    allele_counts = dfa.sum() # genome x total alleles
    for i, genome_faa in enumerate(sorted(genome_faa_paths)):
        print 'Validating genome', i+1, ':', genome_faa, 
        
        ''' Load all alleles present in the genome '''
        genome_alleles = set()
        with open(genome_faa, 'r') as f_faa:
            allele = ''; seq_blocks = []
            for line in f_faa:
                if line[0] == '>': # new sequence encountered
                    seq = ''.join(seq_blocks)
                    if len(seq) > 0:
                        seqhash = __hash_sequence__(seq)
                        allele = seqhash_to_allele[seqhash]
                        genome_alleles.add(allele)
                    seq_blocks = []
                else: # sequence encountered
                    seq_blocks.append(line.strip())
            # process last record
            seq = ''.join(seq_blocks)
            if len(seq) > 0:
                seqhash = __hash_sequence__(seq)
                allele = seqhash_to_allele[seqhash]
                genome_alleles.add(allele)
            
        ''' Check that identified alleles are consistent with the table '''
        genome = genome_faa.split('/')[-1][:-4]
        df_ga = dfa.loc[:,genome]
        table_alleles = set(df_ga.index[pd.notnull(df_ga)])
        test = table_alleles == genome_alleles
        inconsistencies += (1 - int(test))
        print test, len(table_alleles), len(genome_alleles)
    print 'Inconsistencies:', inconsistencies

    
def validate_upstream_table(df_upstream, genome_fna_paths, nr_upstream_fna, limits=(-50,3)):
    '''
    Does a partial validation of the upstream x genome table by checking that
    the recorded upstream sequences are present in the corresponding genome, 
    and counts start codons observed. DOES NOT check the exact location of the
    upstream sequences. See validate_proximal_table() for parameters.
    '''
    validate_proximal_table(df_upstream, genome_fna_paths, nr_upstream_fna, limits, 'upstream')

    
def validate_downstream_table(df_downstream, genome_fna_paths, nr_downstream_fna, limits=(-3,50)):
    '''
    Does a partial validation of the downstream x genome table by checking that
    the recorded downstream downstream are present in the corresponding genome, 
    and counts stop codons observed. DOES NOT check the exact location of the
    downstream sequences. See validate_proximal_table() for parameters.
    '''
    validate_proximal_table(df_downstream, genome_fna_paths, nr_downstream_fna, limits, 'downstream')
    

def validate_proximal_table(df_prox, genome_fna_paths, nr_prox_fna, limits, side):
    '''
    Does a partial validation of the proximal x genome table by checking that
    the recorded proximal sequences are present in the corresponding genome, 
    and counts start codons observed. DOES NOT check the exact location of the
    proximal sequences. TODO: Does not yet support non-fixed length UTRs.
    
    Parameters
    ----------
    df_prox : pd.DataFrame or str
        Either the proximal x genome table, or path to the table as CSV or CSV.GZ
    genome_fna_paths : list
        Paths to FNAs for each genome's contigs
    nr_prox_fna : str
        Path to FNA with non-redundant proximal sequences per gene
    limits : 2-tuple
        Proximal sequence limits specified when extracting upstream 
        or downstream sequences (default (-50,3))
    side : str
        Either "upstream" or "downstream
    '''
    dfp = load_feature_table(df_prox)
    
    ''' Load non-redundant proximal sequences '''
    print 'Loading', side, 'sequences...'
    nr_prox = load_sequences_from_fasta(nr_prox_fna)
            
    ''' Verify present of each proximal sequence within each genome '''
    window = limits[1] - limits[0]
    for g, genome_fna in enumerate(genome_fna_paths):
        genome_contigs = load_sequences_from_fasta(genome_fna)
        genome = genome_fna.split('/')[-1][:-4]
        print g+1, 'Evaluating', genome, genome_fna
        dfp_strain = dfp.loc[:,genome]
        table_prox = dfp_strain.index[pd.notnull(dfp_strain)] # proximal sequences as defined by the table
        table_prox_seqs = {nr_prox[x]:x for x in table_prox} # maps sequences to names
        
        ''' Scan all contigs for detected fixed-length proximal sequences '''
        for contig in genome_contigs.values():
            for i in range(len(contig)):
                segment = contig[i:i+window]
                if segment in table_prox_seqs: 
                    table_prox_seqs.pop(segment)
            rc_contig = reverse_complement(contig)
            for i in range(len(rc_contig)):
                segment = rc_contig[i:i+window]
                if segment in table_prox_seqs: 
                    table_prox_seqs.pop(segment)
                    
        ''' Report undetected proximal sequences '''
        for prox in table_prox_seqs:
            print '\tMissing', table_prox_seqs[prox], 'from', genome
        
    ''' Count start/stop codons among non-redundant proximal sequences '''
    if limits[1] >= 3 and side == 'upstream':
        print 'Computing start codon distribution...'
        if limits[1] == 3:
            get_start = lambda x: x[-3:]
        else:
            get_start = lambda x: x[-limits[1]:-limits[1]+3]
        start_codons = map(get_start, nr_prox.values())
        print collections.Counter(start_codons)
    elif limits[0] <= -3 and side == 'downstream':
        print 'Computing stop codon distribution...'
        if limits[0] == -3:
            get_stop = lambda x: x[:3]
        else:
            get_stop = lambda x: x[-limits[0]-3:-limits[0]]
        stop_codons = map(get_stop, nr_prox.values())
        print collections.Counter(stop_codons)

        
def extract_annotations(genome_gffs, allele_name_file, annotations_out, 
                        batch=100, collapse_alleles=True):
    '''
    For a given allele name file (usually <org>_allele_names.tsv),
    extracts the corresponding PATRIC annotations from original gff files.
    Can load gff files in batches to limit memory usage.
    
    Parameters
    ----------
    genome_gffs : list
        List of paths to GFF files
    allele_name_file : str
        Path to file with allele-protein ID map, usually <org>_allele_names.tsv
    annotations_out : str
        Path to output annotations. Will also create a .tmp intermediate.
    batch : int
        Maximum GFF files to load into at once
    collapse_alleles : bool
        If True, reports annotations at the gene level, using the most common allele
        annotation. Alleles with non-plurality annotation are reported separately.
        If False, reports all unique annotations for all alleles. (default True)
    '''
    
    ''' Copy allele name table '''
    tmp_out = annotations_out + '.tmp'
    shutil.copyfile(allele_name_file, tmp_out)
    
    ''' Iteratively replace protein IDs with annotations from GFFs (in batches) '''
    n_gffs = len(genome_gffs)
    for g in range(0,n_gffs,batch):
        ''' Load annotations for batch of GFFs '''
        annotations = {}
        for gff in genome_gffs[g:g+batch]:
            with open(gff,'r') as f_gff:
                for line in f_gff:
                    data = line.strip().split('\t')
                    if len(data) == 9: 
                        line_annots = data[-1].split(';')
                        line_annots = dict(map(lambda x: x.split('='), line_annots))
                        if 'ID' in line_annots and 'product' in line_annots:
                            product = line_annots['product']
                            product = urllib.unquote(product) # replace % hex characters
                            fid = line_annots['ID']
                            if 'locus_tag' in line_annots:
                                fid += '|' + line_annots['locus_tag']
                            annotations[fid] = product
        print 'Loaded', len(annotations), 'annotations from batch', g+1, '-', min(n_gffs,g+batch)
        
        ''' Incorporate newly loaded annotations '''
        with open(tmp_out, 'r') as f_last:
            with open(tmp_out+'2', 'w+') as f_next:
                for line in f_last:
                    data = line.strip().split('\t')
                    allele = data[0]; fids = data[1:]
                    fids = map(lambda x: annotations[x] if x in annotations else x, fids)
                    fids = list(collections.OrderedDict.fromkeys(fids)) # remove duplicate annotations
                    output = allele + '\t' + '\t'.join(fids)
                    f_next.write(output + '\n')
        shutil.move(tmp_out+'2', tmp_out) # remove last round

    ''' Optionally collapse allele-level annotations to gene-level '''
    if collapse_alleles:
        with open(tmp_out, 'r') as f_last:
            current_cluster = None
            with open(annotations_out, 'w+') as f_next:
                for line in f_last:
                    data = line.strip().split('\t')
                    allele = data[0]
                    cluster =  __get_gene_from_allele__(allele)
                    allele_annots = '\t'.join(data[1:])
                    if current_cluster is None: # initialize first cluster
                        current_cluster = cluster
                        cluster_alleles = [allele]
                        cluster_annots = [allele_annots]
                    elif cluster == current_cluster: # continuing current cluster
                        cluster_alleles.append(allele)
                        cluster_annots.append(allele_annots)
                    else: # start of new cluster
                        most_common_annot, count = collections.Counter(cluster_annots).most_common(1)[0]
                        f_next.write(current_cluster + '\t' + most_common_annot + '\n')
                        for i, annots in enumerate(cluster_annots):
                            if annots != most_common_annot:
                                f_next.write(cluster_alleles[i] + '\t' + annots + '\n')
                        # Initialize next cluster
                        current_cluster = cluster
                        cluster_alleles = [allele]
                        cluster_annots = [allele_annots]
        os.remove(tmp_out)
    else: 
        shutil.move(tmp_out, annotations_out)
    

def load_sequences_from_fasta(fasta, header_fxn=None, seq_fxn=None):
    ''' Loads sequences from a FAA or FNA file into a dict
        mapping headers to sequences. Can optionally apply a 
        function to all header (header_fxn) or to all
        sequences (seq_fxn). Drops the ">" from all headers,
        and removes line breaks from sequences. '''
    header_to_seq = {}
    with open(fasta, 'r') as f:
        header = ''; seq = ''
        for line in f:
            if line[0] == '>': # header line
                if len(header) > 0 and len(seq) > 0:
                    seq = seq_fxn(seq) if seq_fxn else seq
                    header_to_seq[header] = seq
                header = line.strip()[1:]
                header = header_fxn(header) if header_fxn else header
                seq = ''
            else: # sequence line
                seq += line.strip()
        if len(header) > 0 and len(seq) > 0:
            seq = seq_fxn(seq) if seq_fxn else seq
            header_to_seq[header] = seq
    return header_to_seq


def load_feature_table(feature_table):
    ''' 
    Loads DataFrames from CSV, CSV.GZ, PICKLE, or PICKLE.GZ.
    Uses index_col=0 for CSVs. Returns feature_table if provided 
    with anything other than a string.
    '''
    if type(feature_table) == str: # path provided
        if feature_table[-4:].lower() == '.csv' or feature_table[-7:].lower() == '.csv.gz':
            return pd.read_csv(feature_table, index_col=0)
        elif feature_table[-7:].lower() == '.pickle' or feature_table[-10:].lower() == '.pickle.gz':
            return pd.read_pickle(feature_table)
        else:
            return feature_table
    else: # non-string input
        return feature_table
    
    
def reverse_complement(seq):
    ''' Returns the reverse complement of a DNA sequence.
        Supports lower/uppercase and ambiguous bases'''
    return ''.join([DNA_COMPLEMENT[base] for base in list(seq)])[::-1]


def create_feature_name(name, cluster_type, cluster_num, variant_type=None, variant_num=-1):
    ''' 
    Creates the short name of a given feature, defined as 
    <name>_<cluster_type_short><cluster_num>_<variant_type_short><variant_num>
    
    Parameters
    ----------
    name : str
        Header to precede short name
    cluster_type : str
        "cds" or "noncoding", abbreviated as C or T, respectively
    cluster_num : int
        ID of cluster
    variant_type : str
        "allele" or "upstream" or "downstream", abbreviated as A, U, or D,
        respectively. If None, assumes feature refers to whole cluster (default None)
    variant_num : int
        ID of variant. If negative, assumes feature refers to whole cluster (default -1)
    '''
    short_name = name + '_'
    cluster_name_short = CLUSTER_TYPES[cluster_type]
    short_name += cluster_name_short + str(cluster_num)
    if (not variant_type is None) and (variant_num >= 0):
        variant_type_short = VARIANT_TYPES[variant_type]
        short_name += variant_type_short + str(variant_num)
    return short_name


def breakdown_feature_name(feature_name):
    '''
    Reverse function for create_feature_name()
    '''
    data = feature_name.split('_')
    name = '_'.join(data[:-1]); footer = data[-1]
    cluster_type = CLUSTER_TYPES_REV[footer[0]]
    for i in range(1,len(footer)): # identify variant type
        if footer[i].isalpha():
            variant_type = VARIANT_TYPES_REV[footer[i]]
            cluster_num = int(footer[1:i])
            variant_num = int(footer[i+1:])
            return name, clsuter_type, cluster_num, variant_type, variant_num
    return name, cluster_type, cluster_num, None, None
    

def __create_sparse_data_frame__(sparse_array, index, columns):
    ''' 
    Creates a SparseDataFrame from a scipy.sparse matrix by initializing 
    individual columns as SparseSeries, then concatenating them. 
    Sometimes faster than native SparseDataFrame initialization? Known issues: 
    
    - Direct initialization is slow in v0.24.2, see https://github.com/pandas-dev/pandas/issues/16773
    - Empty SparseDataFrame initialization time scales quadratically with number of columns
    - Initializing as dense and converting sparse uses more memory than directly making sparse
    '''
    n_row, n_col = sparse_array.shape
    X = sparse_array.T if n_row < n_col else sparse_array # make n_col < n_row
    sparse_cols = []
    for i in range(X.shape[1]): # create columns individually
        sparse_col = pd.SparseSeries.from_coo(scipy.sparse.coo_matrix(X[:,i]), dense_index=True)
        sparse_cols.append(sparse_col)
    df = pd.concat(sparse_cols, axis=1)
    df = df.T if n_row < n_col else df # transpose back if n_col > n_row originally
    df.index = pd.Index(index)
    df.columns = pd.Index(columns)
    return df


def __load_feature_to_allele__(allele_names):
    ''' Loads feature-to-allele mapping from file, usually <name>_allele_names.tsv. '''
    map_feature_to_gffid = lambda x: '|'.join(x.split('|')[:2])
    feat_to_allele = {}
    with open(allele_names, 'r') as f_all:
        for line in f_all:
            data = line.strip().split('\t')
            allele = data[0]; synonyms = data[1:]
            for synonym in synonyms:
                gff_synonym = map_feature_to_gffid(synonym)
                feat_to_allele[gff_synonym] = allele
    return feat_to_allele
                          
def __get_gene_from_allele__(allele):
    ''' Converts <name>_C#A# or <name>_T#A# allele to 
        <name>_C# gene or <name>_T# transcript. '''
    splitter = VARIANT_TYPES['allele']
    return splitter.join(allele.split(splitter)[:-1])

def __get_header_from_fasta_line__(line):
    ''' Extracts a short header from a full header line in a fasta'''
    return line.split()[0][1:].strip()

def __hash_sequence__(seq):
    ''' Hashes arbitary length strings/sequences '''
    return hashlib.sha256(seq).hexdigest()
            
def __stream_stdout__(command):
    ''' Hopefully Jupyter-safe method for streaming process stdout '''
    process = sp.Popen(command, stdout=sp.PIPE, shell=True)
    while True:
        line = process.stdout.readline()
        if not line:
            break
        yield line.rstrip()
    